{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9bcdbd2-3401-41ad-a83f-830e9346e607",
   "metadata": {
    "id": "d9bcdbd2-3401-41ad-a83f-830e9346e607"
   },
   "source": [
    "# **Applied Machine Learning Homework 5: NLP**\n",
    "**Due May 2,2023 (Tuesday) 11:59PM EST**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "EyLMOVXq-bBX",
   "metadata": {
    "id": "EyLMOVXq-bBX"
   },
   "source": [
    "### Instructions\n",
    "\n",
    "1) Please push the .ipynb and .pdf to Github Classroom prior to the deadline, .py file is optional (not needed).<br>\n",
    "2) Please include your Name and UNI below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "leint4ja8V77",
   "metadata": {
    "id": "leint4ja8V77"
   },
   "source": [
    "# Name: Liang Hu\n",
    "# UNI: lh3057\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70df26be-5638-4b0d-a252-4437eb76aa46",
   "metadata": {
    "id": "70df26be-5638-4b0d-a252-4437eb76aa46"
   },
   "source": [
    "### Natural Language Processing\n",
    "We will train a supervised training model to predict if a tweet has a positive or negative sentiment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0d9a19-25ea-4490-b0e8-7909bcdc3d9d",
   "metadata": {
    "id": "2e0d9a19-25ea-4490-b0e8-7909bcdc3d9d"
   },
   "source": [
    "####  **Dataset loading & dev/test splits**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fafa37c4-c8fc-4697-9bbe-11539d710bf7",
   "metadata": {
    "id": "fafa37c4-c8fc-4697-9bbe-11539d710bf7"
   },
   "source": [
    "**1.1) Load the twitter dataset from NLTK library**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f4ce405-237b-42d2-9c81-25ff28deaf4a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7559,
     "status": "ok",
     "timestamp": 1680552434830,
     "user": {
      "displayName": "Pooja Srinivasan",
      "userId": "13687370184540140629"
     },
     "user_tz": 240
    },
    "id": "5f4ce405-237b-42d2-9c81-25ff28deaf4a",
    "outputId": "eda237da-0b39-4e12-9a55-54623dddfcb4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     /Users/larry_1/nltk_data...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/larry_1/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/larry_1/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('twitter_samples')\n",
    "from nltk.corpus import twitter_samples \n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report\n",
    "# Feel free to import any other packages you need"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41d62ce-3c78-4b6c-9238-111d990d170f",
   "metadata": {
    "id": "c41d62ce-3c78-4b6c-9238-111d990d170f"
   },
   "source": [
    "**1.2) Load the positive & negative tweets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b92fb408-f72a-4c23-acd8-7c944a52edd3",
   "metadata": {
    "id": "b92fb408-f72a-4c23-acd8-7c944a52edd3"
   },
   "outputs": [],
   "source": [
    "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "all_negative_tweets = twitter_samples.strings('negative_tweets.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Vy1K6hxAjfbi",
   "metadata": {
    "id": "Vy1K6hxAjfbi"
   },
   "source": [
    "**1.3) Make a data frame that has all tweets and their corresponding labels**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6611bdc8",
   "metadata": {
    "id": "6611bdc8"
   },
   "outputs": [],
   "source": [
    "# code here\n",
    "df = pd.DataFrame({'tweet': all_positive_tweets + all_negative_tweets,\n",
    "                   'label': ['positive'] * len(all_positive_tweets) + ['negative'] * len(all_negative_tweets)})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xisDSGVanceN",
   "metadata": {
    "id": "xisDSGVanceN"
   },
   "source": [
    "**1.4) Look at the class distribution of the tweets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "xv8oL2_gnYra",
   "metadata": {
    "id": "xv8oL2_gnYra"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "positive    5000\n",
       "negative    5000\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12eae071-fd8a-4a46-9958-0525c635fd88",
   "metadata": {
    "id": "12eae071-fd8a-4a46-9958-0525c635fd88"
   },
   "source": [
    "**1.5) Create a development & test split (80/20 ratio):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f3673db-d7a8-470b-a3d3-f4522cd359b8",
   "metadata": {
    "id": "0f3673db-d7a8-470b-a3d3-f4522cd359b8"
   },
   "outputs": [],
   "source": [
    "X_dev,X_test,y_dev,y_test = train_test_split(df[\"tweet\"],df[\"label\"],test_size = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b23398-e80e-4624-b89e-c02fabfd3f8d",
   "metadata": {
    "id": "32b23398-e80e-4624-b89e-c02fabfd3f8d"
   },
   "source": [
    "#### **Data preprocessing**\n",
    "We will do some data preprocessing before we tokenize the data. We will remove `#` symbol, hyperlinks, stop words & punctuations from the data. You can use the `re` package in python to find and replace these strings. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89d9d69-1640-4583-a7b7-7ec04ccf3310",
   "metadata": {
    "id": "f89d9d69-1640-4583-a7b7-7ec04ccf3310"
   },
   "source": [
    "**1.6) Replace the `#` symbol with '' in every tweet**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5db4dd6d-e775-49d3-96e1-57620c042d46",
   "metadata": {
    "id": "5db4dd6d-e775-49d3-96e1-57620c042d46"
   },
   "outputs": [],
   "source": [
    "X_dev = X_dev.str.replace('#','')\n",
    "X_test = X_test.str.replace('#','')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c4caa8-d71d-46a8-8859-a8e85c56acfe",
   "metadata": {
    "id": "24c4caa8-d71d-46a8-8859-a8e85c56acfe"
   },
   "source": [
    "**1.7) Replace hyperlinks with '' in every tweet**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "I20m5bIZqp9S",
   "metadata": {
    "id": "I20m5bIZqp9S"
   },
   "outputs": [],
   "source": [
    "X_dev = X_dev.str.replace(r'http\\S+', '')\n",
    "X_test = X_test.str.replace(r'http\\S+', '')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492ae463-b611-4292-9ad2-b778856bf8bc",
   "metadata": {
    "id": "492ae463-b611-4292-9ad2-b778856bf8bc"
   },
   "source": [
    "**1.8) Remove all stop words**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "HA-1Qs8Vz4mB",
   "metadata": {
    "id": "HA-1Qs8Vz4mB"
   },
   "outputs": [],
   "source": [
    "for word in stop:\n",
    "    X_dev = X_dev.str.replace(' '+word+' ',' ').replace(' '+word,' ').replace(word+' ',' ')\n",
    "    X_test = X_test.str.replace(' '+word+' ',' ').replace(' '+word,' ').replace(word+' ',' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169bf8ad-f7ba-4e67-a1a0-92fcdd193ab9",
   "metadata": {
    "id": "169bf8ad-f7ba-4e67-a1a0-92fcdd193ab9"
   },
   "source": [
    "**1.9) Remove all punctuations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5fbeaccf",
   "metadata": {
    "id": "5fbeaccf"
   },
   "outputs": [],
   "source": [
    "for punctuation in string.punctuation:\n",
    "    X_dev = X_dev.str.replace(punctuation,' ')\n",
    "    X_test = X_test.str.replace(punctuation,' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f1af18-0c07-4ffb-994e-daead4740a53",
   "metadata": {
    "id": "b2f1af18-0c07-4ffb-994e-daead4740a53"
   },
   "source": [
    "**1.10) Apply stemming on the development & test datasets using Porter algorithm**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "gSPLyHbmxC1q",
   "metadata": {
    "id": "gSPLyHbmxC1q"
   },
   "outputs": [],
   "source": [
    "#code here\n",
    "def stemSentence(sentece):\n",
    "    porter = PorterStemmer()\n",
    "    token_words = word_tokenize(sentece)\n",
    "    stem_sentence = [porter.stem(word) for word in token_words]\n",
    "    return \" \".join(stem_sentence)\n",
    "\n",
    "for index,sentence in X_dev.iteritems():\n",
    "    X_dev[index] = stemSentence(sentence)\n",
    "for index,sentence in X_test.iteritems():\n",
    "    X_test[index] = stemSentence(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687e23ef-dafd-4183-b2f1-86089e281dd8",
   "metadata": {
    "id": "687e23ef-dafd-4183-b2f1-86089e281dd8"
   },
   "source": [
    "#### **Model training**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c40fa44-01ad-4788-98b9-9c8f0c1252ef",
   "metadata": {
    "id": "0c40fa44-01ad-4788-98b9-9c8f0c1252ef"
   },
   "source": [
    "**1.11) Create bag of words features for each tweet in the development dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3d34c009",
   "metadata": {
    "id": "3d34c009"
   },
   "outputs": [],
   "source": [
    "#code here\n",
    "vector = CountVectorizer(stop_words = 'english')\n",
    "X_dev_bow = vector.fit_transform(X_dev)\n",
    "feature_names = vector.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9f20586b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of development dataset bag of words features: (8000, 14309)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of development dataset bag of words features:\", X_dev_bow.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4baf65cd-019b-4ff4-b93c-3ca8cfffca8e",
   "metadata": {
    "id": "4baf65cd-019b-4ff4-b93c-3ca8cfffca8e"
   },
   "source": [
    "**1.12) Train a Logistic Regression model on the development dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3433a6b0-408d-462e-9072-3495b21bc97b",
   "metadata": {
    "id": "3433a6b0-408d-462e-9072-3495b21bc97b"
   },
   "outputs": [],
   "source": [
    "#code here\n",
    "lr = LogisticRegression(random_state=42).fit(X_dev_bow,y_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c16c6f6-7ab2-4d7a-b9dc-098a72381340",
   "metadata": {
    "id": "1c16c6f6-7ab2-4d7a-b9dc-098a72381340"
   },
   "source": [
    "**1.13) Create TF-IDF features for each tweet in the development dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df2fd6f",
   "metadata": {},
   "source": [
    "Approach of splitting data after data processing is efficient, however follow the notebook approach for now for the train test split first and then performing preprocessing steps. \n",
    "\n",
    "Ensure you do the data preprocessing steps on test dataset too in 1.11 and 1.13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7b417843-ffc4-4614-b2ef-964f8ec3e510",
   "metadata": {
    "id": "7b417843-ffc4-4614-b2ef-964f8ec3e510"
   },
   "outputs": [],
   "source": [
    "#code here\n",
    "vector_tf = TfidfVectorizer()\n",
    "X_dev_tf = vector_tf.fit_transform(X_dev)\n",
    "feature_names_tf = vector_tf.get_feature_names()\n",
    "# print(feature_names_tf.shape())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c88920c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of development dataset TF-IDF features: (8000, 14497)\n"
     ]
    }
   ],
   "source": [
    "X_dev_tf_dense = X_dev_tf.toarray()\n",
    "print(\"Shape of development dataset TF-IDF features:\", X_dev_tf_dense.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3c9776-aad9-4eda-b3c2-d9f6b3e52427",
   "metadata": {
    "id": "ea3c9776-aad9-4eda-b3c2-d9f6b3e52427"
   },
   "source": [
    "**1.14) Train the Logistic Regression model on the development dataset with TF-IDF features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b8c7fe8b-61de-4daa-a338-74295a4902ce",
   "metadata": {
    "id": "b8c7fe8b-61de-4daa-a338-74295a4902ce"
   },
   "outputs": [],
   "source": [
    "#code here\n",
    "lr_tf = LogisticRegression(random_state=42).fit(X_dev_tf,y_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0129e7-a0ea-473e-9ad1-667b44a13a92",
   "metadata": {
    "id": "ab0129e7-a0ea-473e-9ad1-667b44a13a92"
   },
   "source": [
    "**1.15) Compare the performance of the two models on the test dataset using a classification report and the scores obtained. Explain the difference in results obtained.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "246c5462",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The performance of bag of words is 0.7525\n",
      "The performance of tf-idf is 0.7515\n"
     ]
    }
   ],
   "source": [
    "X_test_bow = vector.transform(X_test)\n",
    "y_pred_bow = lr.predict(X_test_bow)\n",
    "X_test_tf = vector_tf.transform(X_test)\n",
    "y_pred_tf = lr_tf.predict(X_test_tf)\n",
    "print(f\"The performance of bag of words is {lr.score(X_test_lr,y_test)}\")\n",
    "print(f\"The performance of tf-idf is {lr_tf.score(X_test_tf,y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4a8795d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report for the model trained on bag of words features:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.73      0.80      0.77      1006\n",
      "    positive       0.78      0.70      0.74       994\n",
      "\n",
      "    accuracy                           0.75      2000\n",
      "   macro avg       0.75      0.75      0.75      2000\n",
      "weighted avg       0.75      0.75      0.75      2000\n",
      "\n",
      "Classification report for the model trained on TF-IDF features:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.74      0.77      0.76      1006\n",
      "    positive       0.76      0.73      0.75       994\n",
      "\n",
      "    accuracy                           0.75      2000\n",
      "   macro avg       0.75      0.75      0.75      2000\n",
      "weighted avg       0.75      0.75      0.75      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate classification reports\n",
    "report_bow = classification_report(y_test, y_pred_bow)\n",
    "report_tf = classification_report(y_test, y_pred_tf)\n",
    "print(\"Classification report for the model trained on bag of words features:\\n\", report_bow)\n",
    "print(\"Classification report for the model trained on TF-IDF features:\\n\", report_tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uo7vBVNC-_nK",
   "metadata": {
    "id": "uo7vBVNC-_nK"
   },
   "source": [
    "The performance of bag of words is 0.7405.\n",
    "The performance of tf-idf is 0.748.\n",
    "The bag of words weights more on the high frequency word. On the other hand, the TF-IDF model weights more on the unique word.\n",
    "Using the tf-idf method to represent text data has provided better results compared to the bag of words approach."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
